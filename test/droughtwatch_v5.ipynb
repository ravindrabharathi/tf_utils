{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "droughtwatch-v5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNuuXBL7e5uMtlRNHc8xIyU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrabharathi/tf_utils/blob/active_learning_drought_watch/test/droughtwatch_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNriYzKZ6ElS"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,Concatenate,Lambda,Activation,Input,Dropout\n",
        " \n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        " \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "%matplotlib inline\n",
        "\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5BWxG2y_ELW",
        "outputId": "d2108a0c-04df-4622-b500-fb33538ca3fe"
      },
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mravindra\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "mIUFi9MA-YtE",
        "outputId": "a4f71584-2a80-4a45-b745-f6a2962d453b"
      },
      "source": [
        "run=wandb.init(name='draughtwatch_7', \n",
        "           project='Wandb_Drought_Watch',\n",
        "           notes='Drought Watch dataset with Bands B2-B7, custome ResNet', \n",
        "           tags=['DroughtWatch', 'tf_utils','ResNet'])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.27<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">draughtwatch_7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/ravindra/Wandb_Drought_Watch\" target=\"_blank\">https://wandb.ai/ravindra/Wandb_Drought_Watch</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/ravindra/Wandb_Drought_Watch/runs/16f94ow5\" target=\"_blank\">https://wandb.ai/ravindra/Wandb_Drought_Watch/runs/16f94ow5</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210421_134958-16f94ow5</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRtgIa4WLJq9",
        "outputId": "60e58e5d-18ff-418f-dca8-5d9c2da90449"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/ravindrabharathi/tf_utils@active_learning_drought_watch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ravindrabharathi/tf_utils@active_learning_drought_watch\n",
            "  Cloning https://github.com/ravindrabharathi/tf_utils (to revision active_learning_drought_watch) to /tmp/pip-req-build-fbajlg1l\n",
            "  Running command git clone -q https://github.com/ravindrabharathi/tf_utils /tmp/pip-req-build-fbajlg1l\n",
            "  Running command git checkout -b active_learning_drought_watch --track origin/active_learning_drought_watch\n",
            "  Switched to a new branch 'active_learning_drought_watch'\n",
            "  Branch 'active_learning_drought_watch' set up to track remote branch 'active_learning_drought_watch' from 'origin'.\n",
            "Building wheels for collected packages: tf-utils\n",
            "  Building wheel for tf-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-utils: filename=tf_utils-0.4-cp37-none-any.whl size=8986 sha256=4a30d17d540bd3406dbc563d91bd33cbb5f12e4c7075f6a875ebb671e78c8392\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rcg3_5zm/wheels/80/a8/35/f3e2a85eff1b8cb7e5e54cbd69a209356cf2153f9a4ee67904\n",
            "Successfully built tf-utils\n",
            "Installing collected packages: tf-utils\n",
            "  Found existing installation: tf-utils 0.4\n",
            "    Uninstalling tf-utils-0.4:\n",
            "      Successfully uninstalled tf-utils-0.4\n",
            "Successfully installed tf-utils-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASBLPHIDLjb5"
      },
      "source": [
        "import tf_utils.data as ds \n",
        "import tf_utils.visualize as vz\n",
        "import tf_utils.transform as tfm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WC8V27woTPo",
        "outputId": "a73fddec-cdaf-4bc1-ff7c-130989340bd0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE5wbjZfoQ8n"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro8PbfCut_hk"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMh5pUpCuFB2"
      },
      "source": [
        "dirlist = lambda di: [os.path.join(di, file) for file in os.listdir(di) if 'part-' in file]\n",
        "training_files = dirlist('/gdrive/MyDrive/wandb-dw/data/droughtwatch_data/train/')\n",
        "val_files = dirlist('/gdrive/MyDrive/wandb-dw/data/droughtwatch_data/val/')\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj4EpDYt6iIe",
        "outputId": "f547f337-e954-420d-e45a-d7b9ea382110"
      },
      "source": [
        "train_ds=ds.get_train_ds(training_files,batch_size=ds.batch_size,shuffle=True,distort=True,distort_fn=tfm.aug1)\n",
        "#unlabelled_ds=ds.get_unlabelled_ds()\n",
        "test_ds=ds.get_test_ds(val_files)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 1.0903 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 1.0906 secs\n",
            "Finished 'get_train_ds' in 1.0909 secs\n",
            "Finished 'get_tf_dataset_2' in 0.0680 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.0683 secs\n",
            "Finished 'get_test_ds' in 0.0685 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3y3TzDXOCKci",
        "outputId": "2cf8c847-666c-46c9-ffa0-8284443ef52e"
      },
      "source": [
        "'''\n",
        "for idx, row in enumerate(train_ds):\n",
        "    print (row)\n",
        "    if idx > 1:\n",
        "        break\n",
        "\n",
        "'''        "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor idx, row in enumerate(train_ds):\\n    print (row)\\n    if idx > 1:\\n        break\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m26XDkJi1YyS"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D,  Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ludku7wVG4yD",
        "outputId": "3182e6f1-c04a-439d-f744-69909a0529e2"
      },
      "source": [
        "'''\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same')(inp)\n",
        "  conv_layer=BatchNormalization()(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "'''  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef conv(inp,f=32,k=3):\\n  conv_layer=Conv2D(f,k,use_bias=False,padding='same')(inp)\\n  conv_layer=BatchNormalization()(conv_layer)\\n  conv_layer=Activation('relu')(conv_layer)\\n  return conv_layer\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw9GlcR4wqvJ"
      },
      "source": [
        "import math\n",
        "WEIGHT_DECAY=1.25e-4\n",
        "reg=tf.keras.regularizers.l2(WEIGHT_DECAY)\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "def conv(inp,f=32,k=3):\n",
        "  conv_layer=Conv2D(f,k,use_bias=False,padding='same',kernel_initializer=init_pytorch, kernel_regularizer=reg)(inp)\n",
        "  conv_layer=BatchNormalization(momentum=0.9, epsilon=1e-5)(conv_layer)\n",
        "  conv_layer=Activation('relu')(conv_layer)\n",
        "  return conv_layer\n",
        "def resBlk(inp,f=32,k=3,residual=True) :\n",
        "  res1=conv(inp,f,k)\n",
        "  res1=MaxPooling2D(pool_size=(2,2))(res1)\n",
        "  if residual:\n",
        "    res2=conv(res1,f,k)\n",
        "    res3=conv(res2,f,k)\n",
        "    return res1+res3\n",
        "  else:\n",
        "    return res1  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeeP8Mg9x6cB"
      },
      "source": [
        "def build_model():\n",
        "  f=64\n",
        "  inp=Input(shape=(65,65,6))\n",
        "  layer1=conv(inp,f,3)\n",
        "  res1=resBlk(layer1,f*2,3)\n",
        "  \n",
        "  res2=resBlk(res1,f*4,3,False)\n",
        "  \n",
        "  res3=resBlk(res2,f*8,3)\n",
        "\n",
        "  #res4=resBlk(res3,f*8,3,False)\n",
        "\n",
        "  #res5=resBlk(res4,f*8,3)\n",
        "  \n",
        "  \n",
        "  layer2=tf.keras.layers.GlobalMaxPooling2D()(res3)\n",
        "  layer3=tf.keras.layers.Dense(4, kernel_initializer=init_pytorch, use_bias=False,kernel_regularizer=reg)(layer2)\n",
        "  layer4=Lambda(lambda x: x*0.125)(layer3)\n",
        "  out=Activation('softmax')(layer4)\n",
        "  model=tf.keras.models.Model(inputs=[inp],outputs=[out])\n",
        "  model.summary()\n",
        "  return model "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t3HmqE-464Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad080af9-40c1-4b70-9c3a-f59d437845f5"
      },
      "source": [
        "model=build_model()\n",
        "opt=SGD(lr=0.0001,momentum=0.9,nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,metrics=['accuracy']\n",
        "              )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 65, 65, 6)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 65, 65, 64)   3456        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 65, 65, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 65, 65, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 65, 65, 128)  73728       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 65, 65, 128)  512         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 65, 65, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 32, 32, 128)  0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 128)  147456      max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 128)  147456      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 32, 32, 128)  0           max_pooling2d[0][0]              \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 256)  294912      tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 256)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 512)  1179648     max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 512)  2048        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 512)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 512)    0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 8, 8, 512)    2359296     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 8, 8, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 8, 8, 512)    0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 512)    2359296     activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 8, 8, 512)    2048        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 8, 8, 512)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 8, 8, 512)    0           max_pooling2d_2[0][0]            \n",
            "                                                                 activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 512)          0           tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4)            2048        global_max_pooling2d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 4)            0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 4)            0           lambda[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,576,256\n",
            "Trainable params: 6,571,776\n",
            "Non-trainable params: 4,480\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxUQ6h0mwQB"
      },
      "source": [
        "class Log2wandb_Callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    wandb.log({\n",
        "        \"Epoch\": epoch,\n",
        "        \"Train Loss\": logs[\"loss\"],\n",
        "        \"Train Acc\": logs[\"accuracy\"],\n",
        "        \"Val Loss\": logs[\"val_loss\"],\n",
        "        \"val_acc\": logs[\"val_accuracy\"],\n",
        "        \"LR\":model.optimizer.lr.numpy()\n",
        "        \n",
        "        })\n",
        "        "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edgH88s94pAZ"
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "      \n",
        "      lr=np.interp([epoch],[0, EPOCHS//5,EPOCHS//2,EPOCHS], [0.0001, 0.1, 0.006,0.001])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADezwutCtnWu"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_cpt=ModelCheckpoint(filepath='/gdrive/MyDrive/wandb-dw/best_model2.h5', \n",
        "                          verbose=1, save_best_only=True,monitor='val_accuracy',mode='auto')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI4tvwOmAxmx",
        "outputId": "bbf19174-457e-4d30-908f-af96af7a8f06"
      },
      "source": [
        "import numpy as np\n",
        "batch_size=128\n",
        "EPOCHS=20\n",
        "callback_list=[Log2wandb_Callback(),lr_sched,model_cpt]\n",
        "model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(86317/batch_size), \n",
        "          validation_data=test_ds, validation_steps=np.ceil(10778/batch_size),\n",
        "          callbacks=callback_list,\n",
        "          verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "epoch  1 : setting learning rate to  0.0001\n",
            "  6/675 [..............................] - ETA: 1:12 - loss: 1.3495 - accuracy: 0.5951WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0383s vs `on_train_batch_end` time: 0.0584s). Check your callbacks.\n",
            "675/675 [==============================] - 80s 113ms/step - loss: 1.1686 - accuracy: 0.5997 - val_loss: 1.2393 - val_accuracy: 0.6168\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61682, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 2/20\n",
            "epoch  2 : setting learning rate to  0.025075\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 1.0178 - accuracy: 0.6361 - val_loss: 1.0955 - val_accuracy: 0.5845\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.61682\n",
            "Epoch 3/20\n",
            "epoch  3 : setting learning rate to  0.050050000000000004\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.9210 - accuracy: 0.6789 - val_loss: 0.9437 - val_accuracy: 0.6684\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.61682 to 0.66838, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 4/20\n",
            "epoch  4 : setting learning rate to  0.07502500000000001\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.8761 - accuracy: 0.6983 - val_loss: 1.0668 - val_accuracy: 0.5999\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.66838\n",
            "Epoch 5/20\n",
            "epoch  5 : setting learning rate to  0.1\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.8569 - accuracy: 0.7092 - val_loss: 1.0090 - val_accuracy: 0.6835\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.66838 to 0.68346, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 6/20\n",
            "epoch  6 : setting learning rate to  0.08433333333333334\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.7998 - accuracy: 0.7333 - val_loss: 0.8893 - val_accuracy: 0.6964\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.68346 to 0.69642, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 7/20\n",
            "epoch  7 : setting learning rate to  0.06866666666666668\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.7598 - accuracy: 0.7478 - val_loss: 0.8001 - val_accuracy: 0.7285\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.69642 to 0.72849, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 8/20\n",
            "epoch  8 : setting learning rate to  0.053000000000000005\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.7289 - accuracy: 0.7604 - val_loss: 0.7680 - val_accuracy: 0.7338\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.72849 to 0.73382, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 9/20\n",
            "epoch  9 : setting learning rate to  0.03733333333333334\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.7013 - accuracy: 0.7682 - val_loss: 0.7463 - val_accuracy: 0.7468\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.73382 to 0.74678, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 10/20\n",
            "epoch  10 : setting learning rate to  0.02166666666666668\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.6678 - accuracy: 0.7800 - val_loss: 0.7103 - val_accuracy: 0.7674\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.74678 to 0.76737, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 11/20\n",
            "epoch  11 : setting learning rate to  0.006\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.6318 - accuracy: 0.7913 - val_loss: 0.6779 - val_accuracy: 0.7704\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.76737 to 0.77040, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 12/20\n",
            "epoch  12 : setting learning rate to  0.0055\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.6134 - accuracy: 0.7991 - val_loss: 0.6783 - val_accuracy: 0.7686\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.77040\n",
            "Epoch 13/20\n",
            "epoch  13 : setting learning rate to  0.005\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.6094 - accuracy: 0.7974 - val_loss: 0.6749 - val_accuracy: 0.7702\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.77040\n",
            "Epoch 14/20\n",
            "epoch  14 : setting learning rate to  0.0045000000000000005\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.6024 - accuracy: 0.8015 - val_loss: 0.6669 - val_accuracy: 0.7727\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.77040 to 0.77270, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 15/20\n",
            "epoch  15 : setting learning rate to  0.004\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5904 - accuracy: 0.8072 - val_loss: 0.6703 - val_accuracy: 0.7714\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.77270\n",
            "Epoch 16/20\n",
            "epoch  16 : setting learning rate to  0.0035\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5830 - accuracy: 0.8082 - val_loss: 0.6655 - val_accuracy: 0.7750\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.77270 to 0.77500, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 17/20\n",
            "epoch  17 : setting learning rate to  0.003\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5799 - accuracy: 0.8078 - val_loss: 0.6625 - val_accuracy: 0.7746\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.77500\n",
            "Epoch 18/20\n",
            "epoch  18 : setting learning rate to  0.0025\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5769 - accuracy: 0.8107 - val_loss: 0.6617 - val_accuracy: 0.7762\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.77500 to 0.77619, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n",
            "Epoch 19/20\n",
            "epoch  19 : setting learning rate to  0.002\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5722 - accuracy: 0.8112 - val_loss: 0.6634 - val_accuracy: 0.7744\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.77619\n",
            "Epoch 20/20\n",
            "epoch  20 : setting learning rate to  0.0014999999999999996\n",
            "675/675 [==============================] - 76s 112ms/step - loss: 0.5661 - accuracy: 0.8129 - val_loss: 0.6580 - val_accuracy: 0.7766\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.77619 to 0.77656, saving model to /gdrive/MyDrive/wandb-dw/best_model2.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3c602667d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tcOCYnOkbLL"
      },
      "source": [
        "def lr_schedule():\n",
        "    \n",
        "    def schedule(epoch):\n",
        "      \n",
        "      lr=np.interp([epoch],[20,25,EPOCHS], [0.001,0.006, 0.0001])[0]\n",
        "      print('epoch ', epoch+1, ': setting learning rate to ',lr)\n",
        "      return lr\n",
        "    \n",
        "    return LearningRateScheduler(schedule)\n",
        "\n",
        "lr_sched = lr_schedule()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjBcmd7GruFg",
        "outputId": "3af3b0a0-176f-4178-a65e-e415c9171dde"
      },
      "source": [
        "train_ds=ds.get_train_ds(training_files,batch_size=ds.batch_size,shuffle=True,distort=True,distort_fn=tfm.aug2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distorting...\n",
            "Finished 'get_tf_dataset_2' in 0.2795 secs\n",
            "Finished 'get_tf_dataset_in_batches' in 0.2798 secs\n",
            "Finished 'get_train_ds' in 0.2800 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5UwDgvvUIBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b4b012-8d3e-49f9-c672-f870524f4c3e"
      },
      "source": [
        "import numpy as np\n",
        "batch_size=128\n",
        "EPOCHS=40\n",
        "callback_list=[Log2wandb_Callback(),lr_sched,model_cpt]\n",
        "model.fit(train_ds,epochs=EPOCHS, steps_per_epoch=np.ceil(86317/batch_size), initial_epoch=20,\n",
        "          validation_data=test_ds, validation_steps=np.ceil(10778/batch_size),\n",
        "          callbacks=callback_list,\n",
        "          verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/40\n",
            "epoch  21 : setting learning rate to  0.001\n",
            "  6/675 [..............................] - ETA: 1:14 - loss: 0.7102 - accuracy: 0.7513WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0427s vs `on_train_batch_end` time: 0.0687s). Check your callbacks.\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6749 - accuracy: 0.7681 - val_loss: 0.6583 - val_accuracy: 0.7731\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.77656\n",
            "Epoch 22/40\n",
            "epoch  22 : setting learning rate to  0.002\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6753 - accuracy: 0.7672 - val_loss: 0.6553 - val_accuracy: 0.7731\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.77656\n",
            "Epoch 23/40\n",
            "epoch  23 : setting learning rate to  0.003\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6745 - accuracy: 0.7675 - val_loss: 0.6629 - val_accuracy: 0.7739\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.77656\n",
            "Epoch 24/40\n",
            "epoch  24 : setting learning rate to  0.004\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6740 - accuracy: 0.7678 - val_loss: 0.6671 - val_accuracy: 0.7707\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.77656\n",
            "Epoch 25/40\n",
            "epoch  25 : setting learning rate to  0.005\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6731 - accuracy: 0.7688 - val_loss: 0.6646 - val_accuracy: 0.7703\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.77656\n",
            "Epoch 26/40\n",
            "epoch  26 : setting learning rate to  0.006\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6742 - accuracy: 0.7676 - val_loss: 0.6739 - val_accuracy: 0.7634\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.77656\n",
            "Epoch 27/40\n",
            "epoch  27 : setting learning rate to  0.005606666666666667\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6688 - accuracy: 0.7694 - val_loss: 0.6689 - val_accuracy: 0.7675\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.77656\n",
            "Epoch 28/40\n",
            "epoch  28 : setting learning rate to  0.005213333333333334\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6634 - accuracy: 0.7725 - val_loss: 0.6654 - val_accuracy: 0.7689\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.77656\n",
            "Epoch 29/40\n",
            "epoch  29 : setting learning rate to  0.00482\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6597 - accuracy: 0.7729 - val_loss: 0.6682 - val_accuracy: 0.7656\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.77656\n",
            "Epoch 30/40\n",
            "epoch  30 : setting learning rate to  0.004426666666666667\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6539 - accuracy: 0.7743 - val_loss: 0.6631 - val_accuracy: 0.7706\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.77656\n",
            "Epoch 31/40\n",
            "epoch  31 : setting learning rate to  0.004033333333333333\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6488 - accuracy: 0.7762 - val_loss: 0.6608 - val_accuracy: 0.7706\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.77656\n",
            "Epoch 32/40\n",
            "epoch  32 : setting learning rate to  0.00364\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6462 - accuracy: 0.7768 - val_loss: 0.6642 - val_accuracy: 0.7631\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.77656\n",
            "Epoch 33/40\n",
            "epoch  33 : setting learning rate to  0.003246666666666667\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6407 - accuracy: 0.7787 - val_loss: 0.6553 - val_accuracy: 0.7696\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.77656\n",
            "Epoch 34/40\n",
            "epoch  34 : setting learning rate to  0.0028533333333333336\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6358 - accuracy: 0.7814 - val_loss: 0.6536 - val_accuracy: 0.7705\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.77656\n",
            "Epoch 35/40\n",
            "epoch  35 : setting learning rate to  0.0024600000000000004\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6331 - accuracy: 0.7813 - val_loss: 0.6544 - val_accuracy: 0.7695\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.77656\n",
            "Epoch 36/40\n",
            "epoch  36 : setting learning rate to  0.002066666666666667\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6264 - accuracy: 0.7852 - val_loss: 0.6500 - val_accuracy: 0.7740\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.77656\n",
            "Epoch 37/40\n",
            "epoch  37 : setting learning rate to  0.001673333333333334\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6244 - accuracy: 0.7851 - val_loss: 0.6523 - val_accuracy: 0.7722\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.77656\n",
            "Epoch 38/40\n",
            "epoch  38 : setting learning rate to  0.0012799999999999999\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6207 - accuracy: 0.7863 - val_loss: 0.6472 - val_accuracy: 0.7719\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.77656\n",
            "Epoch 39/40\n",
            "epoch  39 : setting learning rate to  0.0008866666666666667\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6191 - accuracy: 0.7871 - val_loss: 0.6433 - val_accuracy: 0.7735\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.77656\n",
            "Epoch 40/40\n",
            "epoch  40 : setting learning rate to  0.0004933333333333335\n",
            "675/675 [==============================] - 76s 113ms/step - loss: 0.6138 - accuracy: 0.7887 - val_loss: 0.6447 - val_accuracy: 0.7735\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.77656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3c02225650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erylHhUun9v9"
      },
      "source": [
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}